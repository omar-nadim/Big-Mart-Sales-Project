{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The BigMart Sales Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this project is to predict the sales for each product in BigMart. The data for 1559 products across 10 outlets in different cities has been collected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first import all the libraries we are going to need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv, get_dummies\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'Train.csv'\n",
    "Dataset = read_csv(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start by exploring the data and try to make sense of it.\n",
    "Different items from different outlets have been collected, and multiple features have been measured.\n",
    "The output for this data is the price of the item in a particular type of outlet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list and types of the features is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Types = Dataset.dtypes\n",
    "Values_Table = Dataset.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item_Identifier               object\n",
      "Item_Weight                  float64\n",
      "Item_Fat_Content              object\n",
      "Item_Visibility              float64\n",
      "Item_Type                     object\n",
      "Item_MRP                     float64\n",
      "Outlet_Identifier             object\n",
      "Outlet_Establishment_Year      int64\n",
      "Outlet_Size                   object\n",
      "Outlet_Location_Type          object\n",
      "Outlet_Type                   object\n",
      "Item_Outlet_Sales            float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(Types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item_Identifier              8523\n",
      "Item_Weight                  7060\n",
      "Item_Fat_Content             8523\n",
      "Item_Visibility              8523\n",
      "Item_Type                    8523\n",
      "Item_MRP                     8523\n",
      "Outlet_Identifier            8523\n",
      "Outlet_Establishment_Year    8523\n",
      "Outlet_Size                  6113\n",
      "Outlet_Location_Type         8523\n",
      "Outlet_Type                  8523\n",
      "Item_Outlet_Sales            8523\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(Values_Table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there are some missing values. let's confirm that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item_Identifier                 0\n",
      "Item_Weight                  1463\n",
      "Item_Fat_Content                0\n",
      "Item_Visibility                 0\n",
      "Item_Type                       0\n",
      "Item_MRP                        0\n",
      "Outlet_Identifier               0\n",
      "Outlet_Establishment_Year       0\n",
      "Outlet_Size                  2410\n",
      "Outlet_Location_Type            0\n",
      "Outlet_Type                     0\n",
      "Item_Outlet_Sales               0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "Missing_Values_Table = Dataset.isnull().sum()\n",
    "print(Missing_Values_Table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we move any further, let's take care of the missing values.\n",
    "\n",
    "we have some missing values in Item weight, This is very easy to fill as the \n",
    "product with the same identifier must have the same weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for Id in Dataset.Item_Identifier.unique():\n",
    "    Dataset.loc[Dataset['Item_Identifier']==Id] = Dataset.loc[Dataset['Item_Identifier']==Id].fillna(\n",
    "            Dataset.loc[Dataset['Item_Identifier']==Id].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now Let's take care of Item_Fat_Content, we have some LF and low fat values that need to be replaced by Low Fat\n",
    "and Reg value that need to be replaced by Regular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "low fat     112\n",
      "reg         117\n",
      "LF          316\n",
      "Regular    2889\n",
      "Low Fat    5089\n",
      "Name: Item_Fat_Content, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "Item_Counting = Dataset['Item_Fat_Content'].value_counts(ascending=True)\n",
    "print(Item_Counting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset = Dataset.replace(['LF','low fat'],'Low Fat')\n",
    "Dataset = Dataset.replace('reg','Regular')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take care of the missing values of the Outlet_Size,\n",
    "For grocery stores the size will be small Looks like the majority of the Sizes are small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset.Outlet_Size = Dataset.Outlet_Size.fillna('Small')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's recheck for the mssing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item_Identifier              0\n",
      "Item_Weight                  0\n",
      "Item_Fat_Content             0\n",
      "Item_Visibility              0\n",
      "Item_Type                    0\n",
      "Item_MRP                     0\n",
      "Outlet_Identifier            0\n",
      "Outlet_Establishment_Year    0\n",
      "Outlet_Size                  0\n",
      "Outlet_Location_Type         0\n",
      "Outlet_Type                  0\n",
      "Item_Outlet_Sales            0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "Missing_Values_Table = Dataset.isnull().sum()\n",
    "print(Missing_Values_Table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of Unique products we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1559\n"
     ]
    }
   ],
   "source": [
    "Item_Id = Dataset['Item_Identifier'].value_counts(ascending=True)\n",
    "No_Unique_Items = Item_Id.size\n",
    "print(No_Unique_Items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, regarding the Item_Identifier, I don't think it adds any value to the data so I drop it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset = Dataset.drop(['Item_Identifier'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think the Outlet Identifier column could be dropped as it's hight correlated with the outlet location type and outlet type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset = Dataset.drop(['Outlet_Identifier'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Encoding Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regular    3006\n",
      "Low Fat    5517\n",
      "Name: Item_Fat_Content, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(Dataset['Item_Fat_Content'].value_counts(ascending=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are only two value so we can use the LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset['Item_Fat_Content']=encoder.fit_transform(Dataset['Item_Fat_Content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High       932\n",
      "Medium    2793\n",
      "Small     4798\n",
      "Name: Outlet_Size, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(Dataset['Outlet_Size'].value_counts(ascending=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For outlet_size LabelEncoder is most suitable because the sizes are comparable,\n",
    "so  Small:1 Medium:2 High:3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset = Dataset.replace('Small',1)\n",
    "Dataset = Dataset.replace('Medium',2)\n",
    "Dataset = Dataset.replace('High',3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to apply one hot encoding to the remaining columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset = get_dummies(Dataset, columns=['Item_Type'])\n",
    "Dataset = get_dummies(Dataset, columns=['Outlet_Location_Type'])\n",
    "Dataset = get_dummies(Dataset, columns=['Outlet_Type'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move the sales column to the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = Dataset.columns.tolist()\n",
    "cols.insert(29, cols.pop(cols.index('Item_Outlet_Sales')))\n",
    "Dataset=Dataset.reindex(columns=cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Machine Learning Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = Dataset.values\n",
    "X = array[:,0:-1]\n",
    "Y = array[:,-1]\n",
    "\n",
    "validation_size = 0.2\n",
    "seed = 7\n",
    "\n",
    "X_train, X_validation, Y_train, Y_validation = train_test_split(X, Y,\n",
    "        test_size=validation_size, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.543416484472155\n",
      "0.6406342445748947\n"
     ]
    }
   ],
   "source": [
    "# Test options and evaluation metric\n",
    "num_folds = 10\n",
    "seed = 7\n",
    "scoring = 'neg_mean_squared_error'\n",
    "\n",
    "reg = LinearRegression().fit(X_train, Y_train)\n",
    "print(r2_score(Y_validation, reg.predict(X_validation)))\n",
    "\n",
    "GBR = GradientBoostingRegressor(random_state = seed)\n",
    "GBR.fit(X_train,Y_train)\n",
    "print(GBR.score(X_train,Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
